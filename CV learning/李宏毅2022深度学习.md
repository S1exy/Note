## 1.语法初步

### 1.首先是Dataset的导入（数据集的导入

```python
from torch.utils.data import Dataset ,DataLoader
import torch

class MyDataset(Dataset):
    def __init__(self):
        self.data = torch.randn(3,2)
        self.label = torch.tensor([0,1,0])
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self,idx):
        return self.data[idx],self.label[idx]
    
    dataset = MyDataset()

DataLoader = DataLoader(dataset=dataset,batch_size=2,shuffle=True)
```

对这个过程进行举例如上 是一个比较基本的读取资料的过程



### 2.数学运算和一些常见的库函数：

加减乘除都可以进行计算

包括可以进行维度的互换，矩阵）

比如说

- `x = x.transpose(0 , 1)`

  就是将矩阵的第一个维度和第二个维度进行互换

- `x = x.squeeze(1)`

  就是删除掉矩阵的第 1 个维度

- `x.unsqueeze(1)`

  可以在维度 1 的地方新增加一行的矩阵

- 合并矩阵cat，较为复杂提供举例：

```python
x = torch.zeros([2,1,3])
y = torch.zeros([2,3,3])
z = torch.zeros([2,2,3])
w = torch.cat([x,y,z],dim=1)
w.shape
# 本质上里面按第 1 个维度进行了矩阵的合并
>>>torch.Size([2,6,3])
```

### 3.比较重要的一点调整设备的使用

tensors的默认操作是  CPU

可以使用`.to()`的方法来调用调整设备

- CPU：`x = x.to('cpu')`

- GPU:  `x = x.to('cuda')`

### 4.梯度下降的计算和使用过程：

![image-20250203211145933](https://gitee.com/Slexy/picture/raw/master/20250203225400388.png)

```python
x = torch.tensor("...")
# 定义矩阵
z = x.pow(2).sum()
z.backward()
# 反向传播计算梯度
x.grad
```

## 2.实际上去训练一个模型（Network Layers）



包含了三个**步骤**：

- [x] 定义神经网络
- [ ] 定义损失函数
- [ ] 优化器计算并进行优化

![image-20250203213404780](https://gitee.com/Slexy/picture/raw/master/20250203225410406.png)

### 1.定义神经网络

#### 1.Linear Layer（Fully-connected Layer）全连接层？ 

它的语法可以是：`nn.Linear(in_features,out_features)`

本质上的原理：

![image-20250203211800974](https://gitee.com/Slexy/picture/raw/master/20250203225422524.png)

实际上他的本质就是W * x的过程 （向量的乘法）

`in_features`可以代表着输入向量的行or列

`out_features`代表着输出向量的行or列

![image-20250203212114335](https://gitee.com/Slexy/picture/raw/master/20250203225431741.png)

在创建一个 **Layer** 后可以使用 `.weight` 去查看对应的层的参数

#### 2.Non-Linear Activation Functions

1. Sigmoid Activation

   `nn.Sigmoid( --- )` 图像如下:

   ![image-20250203212324734](https://gitee.com/Slexy/picture/raw/master/20250203225451390.png)

2. Re LU Activation

   `nn.ReLU()` 图像如下：

   ![image-20250203212418528](https://gitee.com/Slexy/picture/raw/master/20250203225456905.png)


#### 3.建立一个自己的神经网络模型：

```python
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(10, 32),
            nn.Sigmoid(),
            nn.Linear(10, 1)
        )
        # 以上是一个简单的神经网络模型，包含两个线性层，一个激活函数，一个输出层。


    def forward(self, x):
        return self.net(x)
    # forward函数定义了数据在模型中的流动方向，即数据如何通过模型，从而得到输出结果。
```

### 2.定义损失函数：

#### 1. MSE(Mean Squared Error)：(和正确答案的距离一般用于回归Regression的任务)

`criterion = nn.MSEloss()`

#### 2.CE(Cross Entropy)：一般用于分类Classification的任务

`criterion = nn.CrossEntropyLoss()`



**计算loss最终的结果即为（模型输出 ， 期待获得的值（验证集））**

`loss = Criterion(model_output,expected_value)`



#### 3.选择Optimization Algorithm优化算法：

#### 1.Stochastic Gradient Descent（SGD）：随机梯度下降

`torch.optim.SGD(model.parameters,lr,momeutum = 0)`

`lr` = learning_rate





***实现优化算法的过程：***

- `optimizer.zero_grad()`将前面计算得到的梯度先进行归零的操作
- `loss.backward()` 将算出来的结果回推去算每一层的梯度
- `optimizer.step()` 根据算出来的梯度去调整模型的参数 







最终实现全部整合的一个代码

```python
import torch.nn as nn
from torch.utils.data import Dataset ,DataLoader
import torch

class MyDataset(Dataset):
    def __init__(self):
        self.data = torch.randn(3,2)
        self.label = torch.tensor([0,1,0])
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self,idx):
        return self.data[idx],self.label[idx]
# 以上是一个简单的数据集类，包含了数据和标签，以及数据集的长度和数据获取方法。

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(10, 32),
            nn.Sigmoid(),
            nn.Linear(10, 1)
        )
        # 以上是一个简单的神经网络模型，包含两个线性层，一个激活函数，一个输出层。


    def forward(self, x):
        return self.net(x)
    # forward函数定义了数据在模型中的流动方向，即数据如何通过模型，从而得到输出结果。


file = 'data.txt'  # 数据集文件路径
dataset = MyDataset(file)
tr_set = DataLoader(dataset, batch_size=2, shuffle=True)    # 数据集加载器

model = MyModel().to('cuda')   # 实例化模型，并将模型放到cuda上

criterion = nn.MSELoss()   # 定义损失函数

optimizer = torch.optim.sgd(model.parameters(), lr=0.01)   # 定义优化器


# 训练模型
n_epochs = 10      #定义进行 10 次的epoch训练
for epoch in range(n_epochs):   # 
    for data, label in tr_set:   #分别代表着训练数据和他对应的标签？
        data, label = data.to('cuda'), label.to('cuda')
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, label)
        loss.backward()
        optimizer.step()
        
        
        print(loss.item())
        
# 测试模型
model.eval()  # 将模型调整为测试模式
```

















