

这篇文章主要介绍的就是如何从一张参考图像中提取**视觉特征**（图中给出的比如有艺术风格迁移，外观迁移，风格化 t 2 l 生成，纹理扩展，可控的纹理合成）
这些可以在 figure1 中看到


前文中主要介绍的有就是在之前主流的方法主要使用的是 kv injection 的方法，但是这种方法存在以下几种局限性：
1. 域差异：当内容图像（提供 Q）和风格图像（提供 KV）差异巨大的时候，两者的相似性计算就会变得非常不可靠，这样子就会导致一些错误的特征出现，
2. 误差累计：扩散模型本身是一个迭代的马尔科夫链，在这个过程中，单一的错误会在后续的去噪步骤中不断的累积和放大，最终图像的质量就会降低
3. 架构的限制，kv-injection 发生在 u-net 的残差分支中，这意味着注入的风格信息会受到原本恒等链接的限制，这样可能会导致风格化不充分

本文提出来的创新点，主要是：
1. 注意力蒸馏损失，提出了一个新颖的损失函数，他不再将 kv 特征作为属性，而是通过反向传播来优化合成图像来“蒸馏”参考图像的特征
2. ad 引导采样：将 adloss 作为一个改进的分类器引导，将其集成到扩散模型的去噪采样中，这个样子可以实现更快并且可控的图像生成


关于文章具体的方法步骤：
1. 原本的方法是 kv-injection 通过提取我们需要特征图的 kv 然后提取原本结构图的 q 然后将其进行对其计算注意力，但是这个会发现一个前文中提到的问题，就是这个所谓的风格信息（$K_s, V_s$）知识被注入到了 Unet 的残差分支中，但是由于信息的主干流存在，这个风格的影响力有限，尤其是在风格相差比较大的地方，会引起风格化不充分（当然了这个就是前文的架构的限制之中所提到过的）


关于关键方法的讲解：

**Adloss**

fig 2：
（a）![image.png](https://gitee.com/Slexy/picture/raw/master/20251109232309445.png)
讲了关键的有关于计算 adloss 的步骤，他主要的是改造了 kvinjection 的模式，基于两个并行的自注意力的计算，下面是计算当前的风格化，去查询自己的 k 和 v（代表着目标图像当前的样子）上面这个是老方法 kvinjection 的的事情，那目标图像的 q 去查询风格化图像的 k 和 v，代表我们希望目标图像想要达到的完美风格化的结果，然后这个给到的公式
$$
LAD=∥Self-Attn(Q,K,V)−Self-Attn(Q,Ks,Vs)∥
$$
即为通过计算他们两个逐像素相减的绝对值来计算出他们的 l 1 距离，这就是这篇论文所定义的 adloss

这个相比于传统的 kv-injection 的对比就在下面的这张图：
![image.png](https://gitee.com/Slexy/picture/raw/master/20251110004408497.png)
传统的只将风格信息注入到残差分支中，就是红色箭头的部分，但是 ad 的优势是他不注入任何东西，他计算了一个损失值 $\mathcal{L}_{AD}$ 这个损失可以通过反向传播来更新整个模型，他的梯度可以同时流过残差分支和恒等链接的分支，他不是简单的添加风格，而是让生成的图片本体发生改变，使其的 $Q, K, V$ 特征分布逐渐学习并演变成与风格源的 $K_s, V_s$ 相匹配。

