

这篇文章主要介绍的就是如何从一张参考图像中提取**视觉特征**（图中给出的比如有艺术风格迁移，外观迁移，风格化 t 2 l 生成，纹理扩展，可控的纹理合成）
这些可以在 figure1 中看到


前文中主要介绍的有就是在之前主流的方法主要使用的是 kv injection 的方法，但是这种方法存在以下几种局限性：
1. 域差异：当内容图像（提供 Q）和风格图像（提供 KV）差异巨大的时候，两者的相似性计算就会变得非常不可靠，这样子就会导致一些错误的特征出现，
2. 误差累计：扩散模型本身是一个迭代的马尔科夫链，在这个过程中，单一的错误会在后续的去噪步骤中不断的累积和放大，最终图像的质量就会降低
3. 架构的限制，kv-injection 发生在 u-net 的残差分支中，这意味着注入的风格信息会受到原本恒等链接的限制，这样可能会导致风格化不充分

本文提出来的创新点，主要是：
1. 注意力蒸馏损失，提出了一个新颖的损失函数，他不再将 kv 特征作为属性，而是通过反向传播来优化合成图像来“蒸馏”参考图像的特征
2. ad 引导采样：将 adloss 作为一个改进的分类器引导，将其集成到扩散模型的去噪采样中，这个样子可以实现更快并且可控的图像生成


关于文章具体的方法步骤：
1. 原本的方法是 kv-injection 通过提取我们需要特征图的 kv 然后提取原本结构图的 q 然后将其进行对其计算注意力，但是这个会发现一个前文中提到的问题，就是这个所谓的风格信息（$K_s, V_s$）知识被注入到了 Unet 的残差分支中，但是由于信息的主干流存在，这个风格的影响力有限，尤其是在风格相差比较大的地方，会引起风格化不充分（当然了这个就是前文的架构的限制之中所提到过的）


关于关键方法的讲解：

3.2 **Adloss**

fig 2：
（a）![image.png](https://gitee.com/Slexy/picture/raw/master/20251109232309445.png)
讲了关键的有关于计算 adloss 的步骤，他主要的是改造了 kvinjection 的模式，基于两个并行的自注意力的计算，下面是计算当前的风格化，去查询自己的 k 和 v（代表着目标图像当前的样子）上面这个是老方法 kvinjection 的的事情，那目标图像的 q 去查询风格化图像的 k 和 v，代表我们希望目标图像想要达到的完美风格化的结果，然后这个给到的公式
$$
LAD=∥Self-Attn(Q,K,V)−Self-Attn(Q,Ks,Vs)∥
$$
即为通过计算他们两个逐像素相减的绝对值来计算出他们的 l 1 距离，这就是这篇论文所定义的 adloss

这个相比于传统的 kv-injection 的对比就在下面的这张图：
![image.png](https://gitee.com/Slexy/picture/raw/master/20251110004408497.png)
传统的只将风格信息注入到残差分支中，就是红色箭头的部分，但是 ad 的优势是他不注入任何东西，他计算了一个损失值 $\mathcal{L}_{AD}$ 这个损失可以通过反向传播来更新整个模型，他的梯度可以同时流过残差分支和恒等链接的分支，他不是简单的添加风格，而是让生成的图片本体发生改变，使其的 $Q, K, V$ 特征分布逐渐学习并演变成与风格源的 $K_s, V_s$ 相匹配。

下面的 fig 4 是论文中一个非常重要的一个实验：
![image.png](https://gitee.com/Slexy/picture/raw/master/20251110155340061.png)

他证明了 ad 损失在没有任何的内容指导的情况下，仅靠自身优化的强大能力，
他展示了两大特性：
1. 风格对齐，从上面的儿童手绘图中可以看出，他可以非常精确的蒸馏出并复现参考图像的纹理和风格
2. 空间自适应：他的损失函数不是在简单的复制原图，而是真正的学会了风格（因为在这个实验的过程中他是从多个不同的随机噪声开始，以参考图为目标）这里没有使用任何的内容损失和文本提示，这是一种无条件的风格生成





**3.3 Content-preserving Optimization**
![image.png](https://gitee.com/Slexy/picture/raw/master/20251110161029457.png)

这个就是所谓的内容损失函数 $\mathcal{L}_{content}$
$$
\mathcal{L}_{content} = ∥Q −Qc∥1.
$$

他定义了内容损失为我们正在生成的图像的 Q 向量和内容参考图（结构参考图）的 Qc 向量之间的 L 1 距离，这个损失函数迫使我们生成图像的结构查询和内容参考图的结构查询保持完全一致

最后他定义了一个总损失：
$$
\mathcal{L}_{total} = \mathcal{L}_{AD} + \lambda\mathcal{L}_{content}
$$

其中有一个超参数 $\lambda$，调高 $\lambda$ 内容更重要，同样的结构也更清晰，反过来风格会更重要，图像更抽象（这段需要去看一下 $\lambda$ 是如何设置的（是随机的嘛？还是多次测试出来的））
![image.png](https://gitee.com/Slexy/picture/raw/master/20251110163911913.png)

上面就是这个实验的关于 $\lambda$ 设置的一个对比实验，可以比较直观的看出 $\lambda$ 的大小对于内容和风格之间的影响




3.3**Attention Distillation Guided Sampling**


![image.png](https://gitee.com/Slexy/picture/raw/master/20251110172300764.png)
主要介绍的是如何将这个 ad 损失集成到标注你的扩散采样过程中，来实现 ad 引导采样，
这张图对应的是公式 6 7 8 9：
他整个的过程就是 unet 先看着当前的 zt 然后听从文本提示，通过 ddim 算法，得到了 zt-1，但是在这个过程中根据公式 6 7 8 没有办法在当前将所谓的风格推力 $\nabla \mathcal{L}_{AD}$ 添加进去，因为这个方法很难配置在实验中（图 12）：

![image.png](https://gitee.com/Slexy/picture/raw/master/20251110174531043.png)
