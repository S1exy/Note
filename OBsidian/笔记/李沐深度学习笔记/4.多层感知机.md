## 4.1.多层感知机
### 4.1.1.隐藏层
#### 4.1.1.2.在网络中加入隐藏层
可以在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型，要做到这一点最简单的方法就是把许多全连接层堆叠在一起，每一层都输出到上线的层，直到生成最后的输出，这种架构通常称为_多层感知机_（multilayer perceptron），通常缩写为`MLP`。
![image.png](https://gitee.com/Slexy/picture/raw/master/20250822160921176.png)

#### 4.1.1.3. 从线性到非线性
我们可以根据之前的这张图片来定义一个两层的多层感知机：
**线性层的堆叠问题**

先定义输入 X（每行是一个样本，每列是特征），然后用权重矩阵 W 和偏置 b 得到输出。
如果我们加了一层“隐藏层”，比如：
$$\begin{split}\begin{aligned}
    \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.
\end{aligned}\end{split}$$
这样子是一个隐藏层和一个输出层，但是如果没有非线性激活函数的话，他就会退化成一个$\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}$和$\mathbf{b} = \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)}$的一个单层线性函数：

$$\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}.$$

所以为了发挥多层架构的威力，我们还需要增加一个额外的关键要素，在仿射变换之后对每一个影藏单元应有一个非线性的激活函数，这样子就可以使我们的多层感知机不会退化成线性模型：
$$\begin{split}\begin{aligned}
    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
\end{aligned}\end{split}$$
为了构建更加通用的多层感知机，我们可以继续堆叠这样子的隐藏层，一层叠加一层，从而产生更有表达能力的模型

#### 4.1.2.1. ReLU函数
ReLU是一个修正线性单元，因为它实现简单，同时在各种预测任务中表现良好，给定任何元素x，ReLU函数被定义为该元素与0的最大值：
$$\operatorname{ReLU}(x) = \max(x, 0).$$
~~~python
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
~~~

![image.png](https://gitee.com/Slexy/picture/raw/master/20250830194004532.png)

下面展现的是ReLU函数的导数，我们认定当输入为0时他的导数为0，但是我们可以忽略这种情况，因为输入永远不会是0：

![image.png](https://gitee.com/Slexy/picture/raw/master/20250830194423176.png)


#### 4.1.2.2. sigmoid函数

对于一个定义域在R上的输入，sigmoid函数将输入变换为区间（0,1）上的输出。因此sigmoid通常被称为挤压函数

$$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.
$$


~~~python
y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
~~~

![image.png](https://gitee.com/Slexy/picture/raw/master/20250830194938559.png)

sigmoid函数的导数为下面的公式是：

$$\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).$$

~~~python
# 清除以前的梯度
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
~~~

![image.png](https://gitee.com/Slexy/picture/raw/master/20250830195147452.png)

sigmoid的导数是输入为0时候，而输入在任意方向上远离0点的时候，他的导数会越来越接近0

#### 4.1.2.3. tanh函数

与sigmiod的函数是类似的，tanh（双曲正切）函数也可以将输入压缩到转换区间（-1,1）上，下面给的是tanh函数的公式：

$$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$$
![image.png](https://gitee.com/Slexy/picture/raw/master/20250830195506055.png)


下面给出的是tanh函数的导数：

$$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$$
![image.png](https://gitee.com/Slexy/picture/raw/master/20250830195634269.png)



## 4.2. 多层感知机的从零开始实现

我们现在将要实现一个多层感知机，和之前的结果进行比较，在前面我们先预先进行环境准备，和一些参数的配置：
~~~python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256  # 设置batch_size
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
~~~

### 4.2.1. 初始化模型参数

接下来我们要进行初始化模型的参数：

~~~python
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True) *0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
~~~

其中输入为784 ， 输出为10  ， 隐藏单元为256

### 4.2.2. - 5. 激活函数 .......


ReLU激活函数的代码实现，本质上其实就是先创建一个一模一样的一个零向量矩阵，然后再对他和本体进行max（）的操作进行取最大值
~~~python
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
~~~


模型的实现，因为这是一个简单的多层感知机，所以只需要简单的进行几步就可以啦：
~~~python
def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法
    return (H@W2 + b2)
~~~

损失函数可以直接调用高级api中的函数
~~~python
loss = nn.CrossEntropyLoss(reduction='none')
~~~

训练过程可以参照之前的模版进行训练的填写，当然了使用的优化器使用的是sgd
~~~python
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
~~~


## 4.3. 多层感知机的简洁实现

如果可以使用高级api进行实现多层感知机的话，他就会变得更加简单，也更以容易进行理解

~~~python
import torch
from torch import nn
from d2l import torch as d2l
                                            # 导入模块

net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))     # 设置模型参数，两个全连接层

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01) # 设置模型初始权重，std=0.01，均值=0

net.apply(init_weights);                    # 应用我们设置好的参数


batch_size, lr, num_epochs = 256, 0.1, 10   # 进行训练的操作
loss = nn.CrossEntropyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)



~~~

![image.png](https://gitee.com/Slexy/picture/raw/master/20250831120215430.png)


## 4.4. 模型选择、欠拟合和过拟合

这段先过

## 4.5. 权重衰减

我们通常通过添加一个正则化常熟$\lambda$来描述这种额外损失，下面是他：

$$L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2,$$

在这里还是有一个经典的套路就是这个常数使用是1/2 因为在求导的时候，使用这个可以让求导之后得到的式子他的次数为1。

**他的目的主要的就是惩罚函数的复杂度，较小的这个常数对应较少约束的w，而较大的常熟值对w的约束越大**

## 4.6. 暂退法（Dropout）

### 4.6.1. 重新审视过拟合

泛化性和灵活性之间的这种基本权衡被描述为偏差-方差权衡。线性模型有很高的偏差，他们只能表示一小类的函数，但是这些模型的方差很低，他们在不同的随机数样本上可以得出像是的结果

但是深度神经网络则位于偏差-方差谱的另一端，他与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互，例如，神经网络可能推断尼日利亚和西联汇款一起出现在电子邮件中表示垃圾邮件，但是单独出现这不表示垃圾邮件

### 4.6.2. 扰动的稳健性

暂退法在前向传播的过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术，我们从表面上看来是在训练过程丢弃掉一些神经元，在整个训练过程中的每一次叠代中，标准暂退法包含在计算下一层之前将当前层中的一些节点置0

### 4.6.3. 实践中的暂退法

回看之前的多层感知机的图形，我们如果将暂退法应用到隐藏层，以p的概率将隐藏单元置为0时，结果可以看做是一个只包含原始神经元子集的网络，比如说在这个图中，输出的计算不再依赖于h2和h5，这样子，输出层的计算不能过度依赖于h1-h5中的任何一个元素。

![image.png](https://gitee.com/Slexy/picture/raw/master/20250902145933336.png)

**训练/推理阶段的区别**：

- **训练时**：使用 dropout（随机丢弃）。
    
- **推理时**：不再丢弃，但会把神经元的输出缩放（比如乘以概率 ppp），保证期望一致。

### 4.6.4. 从零开始实现
