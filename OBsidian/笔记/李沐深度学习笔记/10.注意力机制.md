# 10.3. 注意力评分函数

在前面我们介绍了注意力机制的框架，本质上就是，通过一个注意力评分函数，将这个函数的输出结果输入到 Softmax 函数中进行运算，然后得到键与对应的值的概率分布（注意力权重），然后注意力汇聚的输出就是基于这些注意力权重的值的加权和。
上述的算法可以实现下图的注意力机制框架：
![image.png](https://gitee.com/Slexy/picture/raw/master/20250927162554820.png)

在本章中，我主要实现的是谷歌的 attention is all you need 里面的一个实现方案，即下面的公式：
$$
\alpha(\mathbf{q}, \mathbf{k}) = \frac{\mathbf{q}^\top \mathbf{k}}{\sqrt{d}}
$$
他的核心思想是，两个向量的点积可以衡量他们的相似度（在 3 b 1 b 的视频里有提到这个），两个向量的方向越接近，他们的点积越大，所以我们可以通过 q 和 k 之间的相似度，计算出他们的关联度。

至于下面的缩放，在论文中有提到，如果 d 的维度很高，那么点积的结果房差很大，这可能导致 Softmax 函数的梯度变得非常小，这样会让模型的训练变得很困难

下面是他具体的代码：
~~~python
class DotProductAttention(nn.Module):
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens=None):
        d = queries.shape[-1]
        # 使用 bmm (批量矩阵乘法) 高效地计算点积
        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)
        # ... 后续进行 softmax ...
        return self.dropout(self.attention_weights)
~~~
![image.png](https://gitee.com/Slexy/picture/raw/master/20250927164023344.png)
