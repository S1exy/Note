# 6.1. 从全连接层到卷积


## 6.1.1. 不变性

适合于计算机视觉的神经网络架构。

1. _平移不变性_（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
    
2. _局部性_（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。

这就是神经网络相比于多层感知机的优势之所在，因为多层感知机他将所有的输出向量都压为一条线，这个过程中他会丢失掉所有空间信息，他不知道哪一些像素之间是向相邻的，但是，图像中最重要的信息恰恰在于空间结构：
- **局部性 (Locality):** 相邻的像素点关联性很强，它们可能共同构成一条边、一个角或者一块纹理。
    
- **平移不变性 (Translation Invariance):** 图像中的一个物体（比如一只猫），无论它出现在左上角还是右下角，它仍然是一只猫。

全连接网络需要从零开始，费力地去学习这些本应是“常识”的规则，而卷积神经网络（CNN）的设计正是为了天生就解决这两个问题。

## 6.1.3. 卷积
在严格的数学中他符合：
![image.png](https://gitee.com/Slexy/picture/raw/master/20250909210025497.png)
但是实际上：


想象一下，你有一个**“魔法模板”**（这就是卷积核），你想用它来处理一张大图片（输入数据）。

---

 1. 深度学习中的“卷积”（实际上是互相关）

这就像你直接把“魔法模板”盖在图片的左上角。

- 模板上每个位置都有一个数字。
    
- 你把模板上的数字，和你盖住的图片区域里对应的像素值，一个个乘起来。
    
- 最后把所有乘积加在一起，得到一个新数字。
    
- 然后你把模板向右挪一格，重复这个过程。
    

这整个过程是**直接、不翻转**的。

 2. 数学上严格的“卷积”

这个过程多了一个小步骤：

- 在你把“魔法模板”盖在图片上之前，你得先把它**旋转180度**（上下颠倒，左右也颠倒）。
    
- 然后，用这个**翻转后**的模板，去做和上面完全一样的乘积、求和、滑动的操作。
    

**总结一下：** 唯一的区别就是，数学上的卷积多了一步“**先把模板翻转过来**”的操作。



# 6.2. 图像卷积

## 6.2.1. 互相关运算


这里实际上是讲上面这一节的内容，因为实际上卷积表达的运算是互相关运算而并非卷积原酸，具体的区别就在于一个有翻转而另外一个没有，但因为在实际上的神经网络的运算中，他们都要算权重，所以在卷积层之中实际上使用的是互相关运算。

卷积运算的实际图像表示：

![image.png](https://gitee.com/Slexy/picture/raw/master/20250909210811852.png)


输出大小的计算公式为：

$$(n_h-k_h+1) \times (n_w-k_w+1).$$

**实际上的计算公式就是原图像的大小的每一个轴减去卷积核的大小+1然后相乘即可**


详细的代码（细化每个部分的版本：
~~~python
import torch
from torch import nn
from d2l import torch as d2l

def corr2d(X, K):  #@save
    """计算二维互相关运算"""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y
~~~
其中这个切片就是`i:i + h, j:j + w`代表着卷积核，他通过切片的方式切出卷积核，然后和原变量进行相乘，最后得到结果。

## 6.2.2. 卷积层

这边是展现一个在d2l里面，进行实现的一个conv2d的代码可以作为一个参考，其中对于weight的初始化使用的是 `nn.Parameter` 这个他可以告诉pytorch这个参数是需要进行记住的，后面是需要对他进行梯度等操作的，同时对他的初始化是为了展示，而不是简单的进行随机初始化，因为这可能导致梯度消失或者梯度爆炸：
~~~python
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
~~~

所以在正常的情况下可能需要进行特殊的初始化，以保证梯度的正常计算


## 6.2.4. 学习卷积核

代码显示：
~~~python
# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核
conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)

# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），
# 其中批量大小和通道数都为1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2  # 学习率

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # 迭代卷积核
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'epoch {i+1}, loss {l.sum():.3f}')
~~~

我们的这个输入向量他分别是：
（批量大小，通道，高，宽）

## 6.2.5. 互相关和卷积

这个在前面有提到过，就是在这里的卷积并非是在数学意义上的卷积，而是互相关运算，但是因为是机器学习的参数，所以不翻转其实也没有问题

# 6.3. 填充和步幅

填充：
如下图所示，主要解决的是连续卷积层连续丢失像素的问题：
你可以把她看作是假设每一次卷积都丢弃一遍周围的信息，那么就像剥蒜一样，信息在每一次的卷积之中，就会越来越少，这个就是所谓的连续卷积进行丢失的原因

## 6.3.1. 填充

所以我们可以采用填充的方式：
![image.png](https://gitee.com/Slexy/picture/raw/master/20250911143544845.png)

通常，如果我们添加 $Ph$ 行填充（大约一半在顶部，一半在底部）和 $Pw$ 列填充（左侧大约一半，右侧一半），则输出形状将为:

$$(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)。$$

而且这边解释了以下，为什么卷积核的尺寸通常同为奇数，例如3x3，5x5等

首先方便利于padding保持输入和输出尺寸不变。
1. 卷积运算会使输出图像的尺寸变小 
2. 如果我们想让输出尺寸和输入尺寸完全一样，就需要使用填充
3. 当卷积核是奇数的尺寸`k*k`时候，我们可以在图像周围填充`（k-1）/2` 圈像素就可以不多不少的让输出尺寸和输入尺寸完全相等。
   
第二个好处就是可以有明确的中心像素，便于锚定特征位置
比如奇数尺寸的核可以有唯一的中心店，这意味着，输出特征图上 `(i, j)` 位置的值，可以很自然地看作是输入图像上以 `(i, j)` 为中心的区域计算得出的。

（**当然了注意这边的情况是padding=1的情况下即下图**）
![image.png](https://gitee.com/Slexy/picture/raw/master/20250911145454623.png)


下面是一个例子，对于一个 `8x8` 的输入，在使用 `3x3` 卷积核时，只要设置合适的 `padding`，输出尺寸**仍然可以是 `8x8`*。


下面是关于这个在d2l里面这本书定义的一个函数：
~~~python
# 为了方便起见，我们定义了一个计算卷积层的函数。
# 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数
def comp_conv2d(conv2d, X):
    # 这里的（1，1）表示批量大小和通道数都是1
    X = X.reshape((1, 1) + X.shape)
    Y = conv2d(X)
    # 省略前两个维度：批量大小和通道
    return Y.reshape(Y.shape[2:])
~~~
他的作用是，因为在pytorch套件中，他的输入和输出都要求是一个标准的四维向量，格式为：
`(批量大小, 通道数, 高度, 宽度)`。

所以和我们平常的`(高度, 宽度)`。不同所以他将其转换为正确的格式之后再帮我们转化回来，

其中：`X = X.reshape((1, 1) + X.shape)`这个利用了python之中两个元组相加可以直接拼接得到一个新的元组，所以`(1, 1) + (8, 8)` 的结果就是 `(1, 1, 8, 8)`。

另外关于批量大小和通道数，下面做出解释：
1. **批量大小**：代表了图片的颜色深度或者颜色构成，对于一个黑白图片来说，我们只需要一个数值来代表他的亮度，比如说0代表纯黑，255代表纯白，所以一张灰度图只有**1**个通道；但是对于一张彩色图来说，需要三个数值才能表达他，所以一般在RGB下，**他有3个通道**。
2. **批量大小**：![image.png](https://gitee.com/Slexy/picture/raw/master/20250911150659874.png)


## 6.3.2. 步幅

简单来说，步幅就是卷积窗口在输出张量上滑动是如何进行滑动的，可以参考下面的示例图：
![image.png](https://gitee.com/Slexy/picture/raw/master/20250911151122773.png)

现在这边就可以给出一个完整的，计算输出形状的通用公式：

$$\text{输出尺寸} = \left\lfloor \frac{\text{输入尺寸} + 2 \times \text{填充} - \text{核尺寸}}{\text{步幅}} \right\rfloor + 1$$
我们可以用这个公式来快速计算出输出形状



# 6.4. 多输入多输出通道

## 6.4.1. 多输入通道

当输入包含了多个通道时，需要构造和输入数据具有相同输入通道数量的卷积核，以便和输入数据进行互相关运算，在下面就展示了一个两个输入通道进行互相关运算的示例图：
![image.png](https://gitee.com/Slexy/picture/raw/master/20250911152952155.png)

下面展示了一个利用d2l包进行两个输入通道的互相关计算的公式：
~~~python
def corr2d_multi_in(X, K):
    # 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))
~~~
其中这个代码包含：

- **`zip(X, K)`**：这个函数像一个拉链。假设 `X` 有2个通道 `[X_通道1, X_通道2]`，`K` 也有2个通道 `[K_通道1, K_通道2]`。`zip` 会把它们一一配对，变成 `[(X_通道1, K_通道1), (X_通道2, K_通道2)]`。
- **`for x, k in zip(X, K)`**：这是一个循环，依次取出上面配好对的 `(x, k)`。在第一次循环中，`x` 是 `X_通道1`，`k` 是 `K_通道1`；第二次循环亦然。
- **`d2l.corr2d(x, k)`**：对每一对取出的单通道输入 `x` 和单通道核 `k`，执行我们之前学过的标准二维互相关运算。
- **`sum(...)`**：这是最关键的一步。它会把**所有通道**的计算结果（每个结果都是一个二维矩阵）**逐个元素地相加**，最终“压扁”或“融合”成一个**单通道**的二维输出矩阵。

这个就是这个代码包含的内容，其中主要的有zip的内容，和如何进行精妙的取出


## 6.4.2. 多输出通道


在这里我们可以说明一下多输出通道的特点，就是正常来说如果一个卷积核他是输出有一个通道，但是一个卷积核他可能只识别一个特征比如说猫耳朵，那么多个卷积核就是共同去识别出比如说猫脸等特征，这样最终汇聚出最终的结果，所以说最后他的通道数就会越来越多。

教程里有这样子的一句话，“增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度”

他的意思就是说在实际中，比如最刚开始，识别一张大图`1024x1024`，可以先用16个卷积核进行识别，但是他们只能识别基础的简单几何形状，然后生成16张特征图放到下一个卷积层之中，在下面的一个卷积层之中他可能是64个卷积核，他的每一个卷积核的大小都是（`512*512*16`）其中这个16就是上文中提到的多输入通道，他们每一个的参数都各不相同，在计算后相加，然后到下一个卷积层之中，这边表达的就是整个的过程

他的意义是，抽象到具体，整个的过程是一个用空间换通道的过程，在最刚开始 我们关注的是每一个特征的位置，但是在最后我们关注的是特征的组合到底代表了什么


## 6.4.3. `1 X 1` 卷积层

这种卷积层主要是用来拿来在通道的维度上进行计算的，另一个方面上来讲就是他可以帮我们在通道的维度上进行像素级别的全连接层，以实现对于维度本身的升维和降维

$$d_1 = w_{11}c_1 + w_{12}c_2 + w_{13}c_3 + b_1$$

$$d_2 = w_{21}c_1 + w_{22}c_2 + w_{23}c_3 + b_2 $$
这个是对于下面这个图示的公式解释:

![image.png](https://gitee.com/Slexy/picture/raw/master/20250911162625355.png)


他的本质上比如说，可以帮我们进行升维度或者降维，减少计算量或者可以使用 `1x1卷积 + ReLU` 这个组合，在不改变图像高度和宽度的情况下，对不同通道间的信息进行一次非线性的融合，增强网络的表达能力。

# 6.5. 汇聚层

如果我们需要降低表示的空间分辨率，聚集信息，我们可以使用汇聚层来（或者说是池化层）来进行对于缩小对应的空间分辨率
![image.png](https://gitee.com/Slexy/picture/raw/master/20250914000330185.png)


# 6.6. 卷积神经网络（LeNet）

架构如下图所示：

![image.png](https://gitee.com/Slexy/picture/raw/master/20250914000427070.png)

