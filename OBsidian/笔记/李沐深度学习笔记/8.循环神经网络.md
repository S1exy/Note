# 8.1. 序列模型

举了几个例子，我觉得很有意思的是下面这个：
![image.png](https://gitee.com/Slexy/picture/raw/master/20250914134322241.png)

他可能想要表达的意思是，我们需要去处理一个序列数据，我们需要去记住这个序列之中的顺序关系


## 8.1.1. 统计工具

教程中提到了一个显著的序列数据就是股票的价格（如下图所示：

![image.png](https://gitee.com/Slexy/picture/raw/master/20250914134512085.png)

其中，用 $x_t$ 表示价格，即在_时间步_（time step）$t \in \mathbb{Z}^+$ 时，观察到的价格。请注意，$t$  对于本文中的序列通常是离散的，并在整数或其子集上变化。假设一个交易员想在日的股市中表现良好，于是通过以下途径预测 $x_t$：
$$
x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1).
$$
### 8.1.1.1. 自回归模型

![image.png](https://gitee.com/Slexy/picture/raw/master/20250914144822614.png)
$$
P(x_1, \dots, x_T) = \prod_{t=1}^T P(x_t | x_{t-1}, \dots, x_1)
$$
里面存在两个的策略：
1. 第一个策略是使用固定长度的“滑动窗口”，他的优点是他的模型输入永远是固定的，非常简单
   但是他的缺点就是他的记忆力优先，如果输入一段很长的片段，他可能早就忘记了开头，从而无法准确进行下一步的预测
2. 第二个策略就是隐变量自回归模型，如下图所示![image.png](https://gitee.com/Slexy/picture/raw/master/20250914152308250.png)

其实他的本质上就是，根据之前的数据来预测下一个的，公式为：
$$
P(x_1, \dots, x_T) = \prod_{t=1}^T P(x_t | x_{t-1}, \dots, x_1)
$$
即进行**一步一步地预测，后面的预测依赖于前面的所有内容**。

如果我们想要处理离散的对象的话，我们就需要使用分类器而不是回归模型来估计 P。

### 8.1.1.2. 马尔可夫模型

传统的自回归模型认为，未来 x 的状态和过去的所有历史都有关，而马尔科夫假设认为，未来的状态，仅仅与他紧邻的前一个状态有关，和遥远的过去都没有任何的关系

相比于上面的公式，他的公式就会变成下面这样子：
$$
P(x_1, \dots, x_T) = P(x_1) \prod_{t=2}^T P(x_t | x_{t-1})
$$

### 8.1.1.3. 因果关系

这段话的核心思想是，对于大多数序列数据，都存在一个自然的、不可逆的**时间方向**。

你可以把时间看作一条**“单行道”**。我们总是从过去驶向未来。

- **核心观点**：**过去会影响未来，但未来不会影响过去。**
    
- **举例**：
    
    - **合理的（因果的）**：“你昨天熬夜学习（因），所以今天考试考得很好（果）。”
        
    - **不合理的**：“你不能说，因为你明天要中大奖，所以你今天特别开心。”（未来的事件不能成为过去事件的原因）
        

**这对建模意味着什么？** 因此，在构建模型时，我们几乎总是遵循这个自然方向，用过去的数据 `(x_1, ..., x_{t-1})` 来预测未来的数据 `x_t`，即分析 `P(x_t | x_{t-1}, ...)`。这样做不仅符合现实世界的逻辑，也让模型更容易学习到数据背后的规律。


## 8.1.2. 训练

在构造完数据集之后，他的数据的构造是根据几步预测来进行的，例如说他想要进行 `tau` 步的预测，他就会遍历整个序列生成对应的样本特征-标签对：
![image.png](https://gitee.com/Slexy/picture/raw/master/20250917201325085.png)

在这个模型中，他使用的是一个简单的全连接网络：
~~~python
def train(net, train_iter, loss, epochs, lr):
                           trainer = torch.optim.Adam(net.parameters(), lr)
                           for epoch in range(epochs):
                               for X, y in train_iter:
                                   trainer.zero_grad()
                                   l = loss(net(X), y) # 前向传播计算损失
                                   l.sum().backward()  # 反向传播计算梯度
                                   trainer.step()      # 更新模型权重
                               print(f'epoch {epoch + 1}, '
                                     f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')
~~~


## 8.1.3. 预测


但是我们发现在预测未来的过程中，假设我们要预测未来的 10 天，但是我们目前是进行 4 步预测，那么我们在后面的几天就没有办法进行预测，所以在预测的本质上就是进行多步预测，即让模型自言自语，**把自己的预测，当作未来的真实输入，再进行下一步预测。**

但是这样子的话，会出现一个巨大的问题就是，前一步的误差会累积到下一步，这样子的话随着模型的运行，后面的误差会越来越大，直到最后无法接受的程度
![image.png](https://gitee.com/Slexy/picture/raw/master/20250917201803624.png)

# 8.2. 文本预处理

在下面我们介绍文本的常见预处理步骤：
下面是一个读取一篇文章中的一个代码，我们进行逐步的解析:

## 8.2.2. 词元化

首先有如下的代码：
~~~python
def tokenize(lines, token='word'):  #@save
    """将文本行拆分为单词或字符词元"""
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('错误：未知词元类型：' + token)

tokens = tokenize(lines)
for i in range(11):
    print(tokens[i])
~~~

他这边主要是设置了两种拆分的形式，第一种是使用词模式来对每一行进行拆分（因为是英文）所以在这里面，只要使用 `split（）` 就可以了，只需要拆分空格即可，而如果是使用字符串模式，则是将所有的字母一个个的拆分出来，同样的最后返回他的列表

## 8.2.3. 词表

词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。现在，让我们构建一个字典，通常也叫做_词表_（vocabulary），用来将字符串类型的词元映射到从开始的数字索引中。

这个也可以叫做映射表？即 Embedding？

里面含有以下的逻辑：
1. 先分离其中所有的单词元
2. 然后对其进行统计，然后得到语料
3. 然后根据他们唯一词元的出现频率，为其分配一个数字索引个，如果是很少出现的单词，则将其进行移除，降低复杂性
4. 假设语料库中不存在或者已删除的任何词元都会被映射到一个未知词元 `“<unk>”`。


~~~python
class Vocab:  #@save
    """文本词表"""
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # 按出现频率排序
        counter = count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                   reverse=True)
        # 未知词元的索引为0
        self.idx_to_token = ['<unk>'] + reserved_tokens
        self.token_to_idx = {token: idx
                             for idx, token in enumerate(self.idx_to_token)}
        for token, freq in self._token_freqs:
            if freq < min_freq:
                break
            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1

    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]

    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]

    @property
    def unk(self):  # 未知词元的索引为0
        return 0

    @property
    def token_freqs(self):
        return self._token_freqs

def count_corpus(tokens):  #@save
    """统计词元的频率"""
    # 这里的tokens是1D列表或2D列表
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # 将词元列表展平成一个列表
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)
~~~



# 8.3. 语言模型和数据集

在这里似乎他要让我们建立的语言模型是，根据之前给出的文本进行补全后面的文本，语言模型的目标是估计序列的联合概率：
$$
P(x_1, x_2, \ldots, x_T).
$$
例如：
一个好的语言模型会认为 `P("今天天气很好")` 的概率很高而 `P("天气今天很好")` 的概率会低一些。`P("很好天气今天")` 的概率则会非常非常低。


## 8.3.1. 学习语言模型（算是一个解释为什么要使用深度学习方法的一段）

下面介绍一个比较朴素的传统语言统计模型（在 cs 224 n 中似乎有提到）

首先，文章回顾了语言模型的任务：计算一个词序列 `x_t` 出现在历史 `x_{t-1}, ..., x_1` 之后的概率，即 `P(x_t | x_{t-1}, ..., x_1)`。

$$
P(\text{deep}, \text{learning}, \text{is}, \text{fun}) =  P(\text{deep}) P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep}, \text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning}, \text{is}).
$$

这个想法最大的缺陷就是，可能有一些完整的句子在我们俄训练语料库中一次都没有出现过，那么在上面计算的概率一直都是 0，但是这句话在语法上和语义上面都是较为合理的。
所以说直接计数的方法非常脆弱，而且他对于没见过的合理序列可能会给出错误的零概率，该类模型不具有**泛化**能力。

然后提到了另外一个的统计模型：**n-gram模型**

为了解决历史太长导致组合爆炸和数据稀疏的问题，人们因此提出了一种简化的马尔科夫假设，即这个模型，他的公式如下所示：
$$
P(x_t | x_1, \dots, x_{t-1}) \approx P(x_t | x_{t-n+1}, \dots, x_{t-1})
$$


这个公式说明再这个模型里面，他不再查看全部的历史，而是做了一个近似，即认为当前词的概率只和他前面的 n-1 个词有关：

- **Bigram (n=2)**: 概率只与前 1 个词有关。`P(machine | time)`
- **Trigram (n=3)**: 概率只与前 2 个词有关。`P(by | time, machine)`


## 8.3.3. 自然语言统计

接下来我们在真实数据集上进行自然语言统计，并打印出其中最常用的几个单词：
我们可以看到下面是这个的词频图：
![image.png](https://gitee.com/Slexy/picture/raw/master/20250917210632785.png)


下面这个则是验证如果是多个词进行汇集是不是同样适用这个方法：

![image.png](https://gitee.com/Slexy/picture/raw/master/20250917210640440.png)

这几张图片主要都遵循着少数常见，多数罕见的原则，这位传统的直接计数的语言模型带来了巨大的挑战，则引入了可以学习和泛化的深度学习。


### 8.3.4.2. 顺序分区
还有一个内容则是关于原始读取序列的问题：
	????









# 8.4. 循环神经网络


