
## 3.2线性回归的从0开始实现：

### 3.2.1生成数据集

下面这段代码主要是用于生成一组线性回归的模拟数据：

```Python
def synthetic_data(w, b, num_examples):  # @save
    """生成y=Xw+b+噪声"""
    X = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
```

其中主要有几个比较重要的部分：

1. 首选是 `torch.matmul()` 这个函数用于进行矩阵的乘法，即点乘
2. 其次是 `torch.normal()` 这个函数用于进行矩阵的构造，他的三个参数分别为：

（均值，方差，形状）刚好对应这个里面的0和0.01还有x，y的形状

  

### 3.2.2读取数据集

下面这一段代码是一个关于读取数据集的一个示例

```Python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # 这些样本是随机读取的，没有特定的顺序
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
```

其中他的逻辑是首先构建一个随机打乱的一个表，然后从表中进行索引的读取，构建随机的小数据集变成一个合理大小的一个小批量然后带入到模型中进行计算

1. 首先就是`random.shuffle（)` 他本身可以随机打乱一个列表，方便构建一个随机打乱的一个索引表
2. 然后就是 `yield` 这个关键字，这个是有关于ai对于他的解释



但其实他最主要的作用就是节省内容，相当于在函数中打了一个断点，调用一次断点一次，下次调用的时候再从这个地方开始，这样子就可以节省内容，将函数变成一个生成器

  
![image.png](https://gitee.com/Slexy/picture/raw/master/20250815135130194.png)

  

**但是实际上这个还是有个很大的问题就是，他本身需要大量的内存，故实际上大多数实际上的训练还是使用的是内置的文件读取框架**

  

### 3.2.6定义优化算法

```C
def sgd(params, lr, batch_size):#@save
"""小批量随机梯度下降"""
	with torch.no_grad():
		for paramin params:
        param -= lr * param.grad / batch_size
        param.grad.zero_()
```

里面有这句话就是

![image 1.png](https://gitee.com/Slexy/picture/raw/master/20250815135222997.png)


因为 pytorch 内内置的计算梯度的函数算出的是对于所有变量的梯度之和而不是平均梯度，所以说应该要进行……

  

## 3.3线性回归的简洁实现

### 3.2.2读取数据集

```Python
def load_array(data_arrays, batch_size, is_train=True):  # @save
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)
```

其中他的方式主要是：

1. `data.TensorDataset()` 他的作用是将多个Tensor打包成一个TensorDataset的对象，方便于后续的调用  
    这个TensorDataset对象返回的示例如下：  
    
    ![image 2.png](https://gitee.com/Slexy/picture/raw/master/20250815135119811.png)

    
2. `data.DataLoader(dataset, batch_size, shuffle=is_train)` 主要的作用是返回一个Dataloader对象，可以按批次读取数据集中的数据，shuffle主要控制的是是否进行打乱

  
这段代码是用来把特征 (`features`) 和标签 (`labels`) 封装成可迭代的小批量数据（DataLoader），用于训练或评估 PyTorch 模型。  
  

### 3.3.3定义模型

  

全连接层：

```Python
# nn是神经网络的缩写
from torch import nn
net = nn.Sequential(nn.Linear(2, 1))
```

  

在后续的模型定义中，我们一般都用的是也是`Sequential()` 这个函数对模型的层进行封装，将多个层串联在一起

  

而`Linear` 是最为简单的层，他做的就是对每一个输入都通过点乘得到每个的输出

  

### 3.3.4初始化模型参数

```Python
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)
```

可以直接访问参数来设定这个模型的w和b的初始值

  

### 3.3.x 定义优化器&损失函数&训练的代码

  

损失函数:

计算均方误差使用的是MSELoss类，也称为平方范数。 默认情况下，它返回所有样本损失的平均值。  
`loss = nn.MSELoss()`

  

定义优化器：

这里我们实例化一个SGD，随机梯度下降的优化器的优化器，里面可以设置优化算法所需的超参数字典  
`trainer = torch.optim.SGD(net.parameters(), lr=0.03)`

  

训练：

```Python
num_epochs = 3
for epochin range(num_epochs):
for X, yin data_iter:
        l = loss(net(X) ,y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch{epoch + 1}, loss{l:f}')
```

训练代码中主要有几点：

1. 读取训练集和对应的标签
2. 计算损失
3. 将梯度归0
4. 进行反向传播计算梯度
5. 通过梯度优化模型中的参数
6. 在测试集（验证集）中进行验证超参数
7. 最后输出每个epoch的loss和准确率

  

  

## 3.4 Softmax回归

  

### 3.4.2网络架构

![](https://gitee.com/Slexy/picture/raw/master/20250815135105301.png)

Softmax回归也是一个单层神经网络，计算每一个输出取决于所有输入，所以这也是一个全连接层

![image 4.png](https://gitee.com/Slexy/picture/raw/master/20250815135105301.png)


  

### 3.4.4 Softmax运算

Softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时可以让模型保持可导的性质。

  

$\hat{\mathbf{y}} = \text{softmax}(\mathbf{o}) \quad \text{其中} \quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$

  

Softmax运算不会改变未规范化的预测之间的大小次序，只会确定分配给每个类别的概率。

  

### 3.4.6 损失函数

下面这个就是采用的交叉熵损失函数

$$l(\mathbf{y}, \hat{\mathbf{y}}) = -\sum_{j=1}^{q} y_j \log \hat{y}_j.$$

由于yi是一个长度为q的独热编码向量，故如果yi预测不正确的选项这一项将会为0，故所有的都是基于正确的选项进行计算的，并且yi是预测的概率，所以对数不会大于0.并且由于log函数的特性，这个损失函数会狠狠地**惩罚**任何概率过于低的样例，可以参考下面的图：

![image 5.png](https://gitee.com/Slexy/picture/raw/master/20250815135017086.png)

这个惩罚可以推动模型将正确类别的预测概率拉高

  

==**接下来是损失函数的推导，将前面带入损失函数中，进行化简：**==

$$\begin{split}\begin{aligned}  
l(\mathbf{y}, \hat{\mathbf{y}}) &= - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\  
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\  
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.  
\end{aligned}\end{split}$$

推导过程中主要有以下的几步：

1. 拆开 log：

$$= -\sum_{j=1}^q y_j[o_j - \log\sum_{k=1}^q \exp(o_k)]$$

1. 拆成两部分：

$$= \log\sum_{k=1}^q\exp(o_k)-\sum_{j=1}^q y_j o_j$$

当然了在这其中有一步就是

![image 6.png](https://gitee.com/Slexy/picture/raw/master/20250815135037100.png)


但是由于yi是独热向量，并且与他相关的第二项中并没有含有j相关的变量，yi的和可以为1，第二项由于是oj与j有关，故没法将其变为=1的形式

  

（第二项不能直接化成Oj是因为在推导时j是遍历所有类别的，只有在独热向量的数值计算时才会发生退化）

  

  

**接下来我们要解决的是对于损失函数求导的问题，下面是最终得到的结果：**

$$\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.$$

  

我们分别拆出两项对其进行求导：

**第一项：**

$$\frac{\partial \log \sum_{k=1}^q e^{o_k}} {\partial o_j}  
= \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)}  
= \mathrm{softmax}(\mathbf{o})_j$$

其中他的里面有关于eok对于eoj进行求导，因为是对oj求偏导，故只有在k = j时，这两项才有关，故这一项可以直接变成eoj，即上面的公式的分子

  

**第二项：**

$$\frac{\partial}{\partial o_j} \left(-\sum_{i=1}^q y_i o_i\right) = -y_j$$

关于这一项其实主要还是与上面相同，yi关于oj求偏导不变，而oj同样只有在j=k的时候生效，故求导后即为这个结果。

## 3.5 图像分类数据集

### 3.5.1.读取数据集

```python
# 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，
# 并除以255使得所有像素的数值均在0～1之间
trans = transforms.ToTensor()  
mnist_train = torchvision.datasets.FashionMNIST(
    root="../data", train=True, transform=trans, download=True)
mnist_test = torchvision.datasets.FashionMNIST(
    root="../data", train=False, transform=trans, download=True)
```

这个里面主要有几个知识点，分别是
1. `trans = transforms.ToTensor()`是构建了一个转换器函数，将数据集转换为Tensor的格式
2. `mnist_train = torchvision.datasets.FashionMNIST(root="../data", train=True, transform=trans, download=True)`
   则是构建了一个下载数据集的函数，其中`train`选项是选择是否是用于训练，`transform`选项是选择如何进行数据的转换，这个就可以用上之前构建的转换器的函数，`download`选项则是选择是否进行下载，他会检查目录下是否有文件，如果有不会执行，如果没有的话她会帮你从网站上拉取文件进行下载的操作


### 3.5.2读取小批量数据


```python
batch_size = 256

def get_dataloader_workers():  #@save
    """使用4个进程来读取数据"""
    return 4

train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,
                             num_workers=get_dataloader_workers())
```

在这之间我们使用了pythorch中自带的数据迭代器dataloader，同时在`num_worker`中可以指定同时使用的线程数，在这个代码中使用的是四个进程用于读取数据，不过同样的不需要写一个函数来，实际上你直接写 `num_workers=4` 就可以了。

### 3.5.3整合所有组件

接下来我们直接定义一个函数获取数据集，并返回训练集和验证集的数据迭代器
```python
def load_data_fashion_mnist(batch_size, resize=None):  #@save
    """下载Fashion-MNIST数据集，然后将其加载到内存中"""
    trans = [transforms.ToTensor()]
    if resize:
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(
        root="../data", train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root="../data", train=False, transform=trans, download=True)
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                            num_workers=get_dataloader_workers()),
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers()))
```
在这里面分为几步

1. 首先还是`ToTensor()`将数据转换为Tensor的格式
2. 如果传入了resize参数，他会将`Resieze`这个操作放置在`ToTensor()`前面执行，`trans.insert(0, transforms.Resize(resize))`函数的作用就是将某一个操作放在在另一个操作的什么位置上
3. `transforms.Resize(resize)`操作是将图片大小改成 resize×resize 
4. `transforms.Compose(trans)`则是将这些操作全部都进行组合起来，形成一个任务链，一步步的进行执行，这也是为什么刚开始的代码中是`trans = []`的形式了，因为他本身不是一步操作，而是一整串的任务链
5. 接下来的读取数据集的操作就显而易见了，因为在之前已经学习过了

## 3.6. softmax回归的从零开始实现

在这一节中，我们默认数据迭代器的批量大小为256，`batch_size = 256`
### 3.6.1. 初始化模型参数
~~~python
num_inputs = 784  # 因为这个数据集的参数28 * 28 的图像，所有我们把他看做是长度为784的向量，故输入的数量为 28 * 28 = 784
num_outputs = 10

W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)
~~~
后面则是设置权重和偏置，权重还是和之前一样按照正态分布 0 为均值 0.01为方差进行设置
### 3.6.2. 定义softmax操作

回顾一下`sum`运算符是如何沿着向量的特定维度进行工作的
~~~python
X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
X.sum(0, keepdim=True), X.sum(1, keepdim=True)
~~~
其中输出的结果是
~~~text
tensor([[5., 7., 9.]]),
 tensor([[ 6.],
         [15.]])
~~~

`sum`的第一个参数代表着沿哪一个行or列进行求和，而第二个参数`keepdim`则代表是否保留维度，默认值为否，即求和完之后会降低维度（消失一个维度）而这里选择为是，即为保留当前的维度


接着是定义Softmax的计算

$$\mathrm{softmax}(\mathbf{X})_{ij} = \frac{\exp(\mathbf{X}_{ij})}{\sum_k \exp(\mathbf{X}_{ik})}.$$
~~~python
def softmax(X):
    X_exp = torch.exp(X)                       # 1. 对输入矩阵逐元素取指数
    partition = X_exp.sum(1, keepdim=True)     # 2. 对每行求和（保持二维形状）
    return X_exp / partition                   # 3. 每个元素除以该行的总和
~~~

本质上就是首先先对每一个参数进行ex的操作，然后按行求和（沿着列求和）完之后进行除法操作之后得到最终的结果，但是在这里需要注意需要保留`keepdim=True`因为如果没有保留的话，结果是一个一维向量，没有办法和原矩阵进行相处，而保留维度后，可以和X_exp在列方向进行广播扩展匹配

不过会有数值稳定性的问题，即矩阵中非常大或者非常小的元素可能会造成数值的溢出或者下溢，故一般的操作是将每一行都减去最大值，公式改变为下面的式子，这样子就可以进行稳定的计算了：
$$\mathrm{softmax}(x)_i = \frac{\exp(x_i - \max_j x_j)}{\sum_k \exp(x_k - \max_j x_j)}$$

### 3.6.3. 定义模型

~~~python
def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
~~~
这个的本质上其实就是计算Wx + b之后传入之前写好的Softmax函数

### 3.6.4. 定义损失函数

接下来实现之前说过的交叉熵损失函数
~~~python
def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y])

cross_entropy(y_hat, y)
~~~

其中这个里面有一个pytorch的高级索引的用法，例如：
~~~python
y = torch.tensor([0, 2])
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y_hat[[0, 1], y]
~~~
其中`y_hat[[0, 1], y]` 等价于 `y_hat[[0,1], [0,2]]`所以在原本的代码中，他其实就是取每一个样例的真实的y那一项

### 3.6.5. 分类精度
现在主要在计算正确预测数量和总预测量的比，下面的代码是计算预测正确的数量的函数：
~~~python
def accuracy(y_hat, y):  #@save
    """计算预测正确的数量"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
~~~
代码中首先是判断`y_hat`的向量维度是否是1，如果不是的话就使用`argmax（axis = 1）`来判断最大的预测是哪一个，然后使用` = = `和正确的真实元素进行比较，但是需要注意要转换一下元素类型，因为` = = `运算符对于数据的类型比较敏感

### 3.6.6. 训练
~~~python
def train_epoch_ch3(net, train_iter, loss, updater):  #@save
    """训练模型一个迭代周期（定义见第3章）"""
    # 将模型设置为训练模式
    if isinstance(net, torch.nn.Module):
        net.train()
    # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # 使用定制的优化器和损失函数
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练精度
    return metric[0] / metric[2], metric[1] / metric[2]
~~~
其中代码中是：
1. `if isinstance(net, torch.nn.Module):`
   这个主要是判断`net`是否是`nn.module`，如果是的话就会切换到训练模式
2. `metric = Accumulator(3)`
   这个主要是建立一个累加器，装三个数：
   1.损失和
   2.预测正确的样本数
   3.样本总数
3. 
   ![image.png](https://gitee.com/Slexy/picture/raw/master/20250817140711398.png)


最终可以得到类似于这样子的画图效果：
![image.png](https://gitee.com/Slexy/picture/raw/master/20250817141224452.png)


### 3.6.7. 预测

~~~python
def predict_ch3(net, test_iter, n=6):  #@save
    """预测标签（定义见第3章）"""
    for X, y in test_iter:
        break   # 只取一个批次
    trues = d2l.get_fashion_mnist_labels(y)
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
    d2l.show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])

predict_ch3(net, test_iter)
~~~

## 3.7. softmax回归的简洁实现
~~~python
# PyTorch不会隐式地调整输入的形状。因此，
# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
~~~
其中有一些：

1. 这边主要是先记下来定义模型，是由一个展平层和一个线性层组成
2. 然后建立一个函数，让他设置线性层w的默认参数为均值0，标准差为0.01的正态分布初始化权重
   **当然了这个小权重的初始化可以帮助训练初期稳定，不会出现梯度爆炸**
   
   当然了在这个函数中 `if type(m) == nn.Linear:`的作用是对于这个权重只对于全连接层生效，避免对其他层进行误操作

### 3.7.2. 重新审视Softmax的实现

在之前我们主要讲了在Softmax中如何做数值稳定的计算，但是现在我们可以将softmax和交叉熵两者合二为一进行一起计算，这样子的话可以增强数据的稳定性

$$\begin{split}\begin{aligned}
\hat y_j & =  \frac{\exp(o_j - \max(o_k))\exp(\max(o_k))}{\sum_k \exp(o_k - \max(o_k))\exp(\max(o_k))} \\
& = \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}.
\end{aligned}\end{split}$$
在减法和规范化步骤之后，可能有些具有较大的负值。 由于精度受限，将有接近零的值，即下溢（underflow）。 这些值可能会四舍五入为零，并且使得的值为`-inf`。 反向传播几步后，我们可能会发现自己面对一屏幕可怕的`nan`结果。

$$\begin{split}\begin{aligned}
\log{(\hat y_j)} & = \log\left( \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}\right) \\
& = \log{(\exp(o_j - \max(o_k)))}-\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)} \\
& = o_j - \max(o_k) -\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)}.
\end{aligned}\end{split}$$
在这里他直接把log 和 exp这两步结合起来，可以避免反向传播中的数值稳定性问题，因为log（exp（））这两步的计算被自身互相抵消了

~~~python
loss = nn.CrossEntropyLoss(reduction='none')
~~~
在上面的代码中，就是添加一个参数量，让我们可以观看每个样本的损失，下面是一个例子：
~~~python
loss = nn.CrossEntropyLoss(reduction='none')

# logits: batch_size=3, num_classes=4
x = torch.tensor([[2.0, 1.0, 0.1, 0.1],
                  [0.1, 0.2, 3.0, 0.4],
                  [1.0, 2.0, 0.5, 0.3]])
y = torch.tensor([0, 2, 1])   # 真实类别

print(loss(x, y))
~~~
在这里面reduction有三种超参数：
如果你用 `reduction='mean'`，输出就会变成`tensor(0.7082)`（三个值的平均）
如果是 `reduction='sum'`，输出 `tensor(2.1246)`（三个值的和）
如果是 `reduction='none'`，输出 `tensor([0.4170, 0.2906, 1.4170])`（三个值本身）

### 3.7.3. 优化算法
紧接着我们就可以定义优化器啦，这边我们采用的是随机梯度下降的优化器：
~~~python
trainer = torch.optim.SGD(net.parameters(), lr=0.1)
~~~
然后进行模型的训练即可