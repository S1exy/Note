# 线性回归神经网络

## 3.2线性回归的从0开始实现：

### 3.2.1生成数据集

下面这段代码主要是用于生成一组线性回归的模拟数据：

```python
def synthetic_data(w, b, num_examples):  # @save
    """生成y=Xw+b+噪声"""
    X = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
```

其中主要有几个比较重要的部分：

1. 首选是    `torch.matmul()`   这个函数用于进行矩阵的乘法，即点乘
2. 其次是    `torch.normal()`   这个函数用于进行矩阵的构造，他的三个参数分别为：

（均值，方差，形状）刚好对应这个里面的0和0.01还有x，y的形状

### 3.2.2读取数据集

下面这一段代码是一个关于读取数据集的一个示例

```python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # 这些样本是随机读取的，没有特定的顺序
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
```

其中他的逻辑是首先构建一个随机打乱的一个表，然后从表中进行索引的读取，构建随机的小数据集变成一个合理大小的一个小批量然后带入到模型中进行计算

1. 首先就是`random.shuffle（)` 他本身可以随机打乱一个列表，方便构建一个随机打乱的一个索引表
2. 然后就是 `yield` 这个关键字，这个是有关于ai对于他的解释

![image.png](%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20245f0d9df6f18075a424cb5572014c1c/image.png)

但其实他最主要的作用就是节省内容，相当于在函数中打了一个断点，调用一次断点一次，下次调用的时候再从这个地方开始，这样子就可以节省内容，将函数变成一个生成器

**但是实际上这个还是有个很大的问题就是，他本身需要大量的内存，故实际上大多数实际上的训练还是使用的是内置的文件读取框架**

### 3.2.6定义优化算法

```c
def sgd(params, lr, batch_size):#@save
"""小批量随机梯度下降"""
	with torch.no_grad():
		for paramin params:
        param -= lr * param.grad / batch_size
        param.grad.zero_()
```

里面有这句话就是

![image.png](%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20245f0d9df6f18075a424cb5572014c1c/image%201.png)

因为 pytorch 内内置的计算梯度的函数算出的是对于所有变量的梯度之和而不是平均梯度，所以说应该要进行……

## 3.3线性回归的简洁实现

### 3.2.2读取数据集

```python
def load_array(data_arrays, batch_size, is_train=True):  # @save
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)
```

其中他的方式主要是：

1. `data.TensorDataset()`  他的作用是将多个Tensor打包成一个TensorDataset的对象，方便于后续的调用
这个TensorDataset对象返回的示例如下：

    
    ![image.png](%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20245f0d9df6f18075a424cb5572014c1c/image%202.png)
    
2. `data.DataLoader(dataset, batch_size, shuffle=is_train)` 主要的作用是返回一个Dataloader对象，可以按批次读取数据集中的数据，shuffle主要控制的是是否进行打乱

这段代码是用来把特征 (`features`) 和标签 (`labels`) 封装成可迭代的小批量数据（DataLoader），用于训练或评估 PyTorch 模型。

### 3.3.3定义模型

全连接层：

```python
# nn是神经网络的缩写
from torch import nn
net = nn.Sequential(nn.Linear(2, 1))
```

在后续的模型定义中，我们一般都用的是也是`Sequential()` 这个函数对模型的层进行封装，将多个层串联在一起

而`Linear` 是最为简单的层，他做的就是对每一个输入都通过点乘得到每个的输出

### 3.3.4初始化模型参数

```python
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)
```

可以直接访问参数来设定这个模型的w和b的初始值

### 3.3.x 定义优化器&损失函数&训练的代码

损失函数:

计算均方误差使用的是MSELoss类，也称为平方范数。 默认情况下，它返回所有样本损失的平均值。
`loss = nn.MSELoss()`

定义优化器：

这里我们实例化一个SGD，随机梯度下降的优化器的优化器，里面可以设置优化算法所需的超参数字典
`trainer = torch.optim.SGD(net.parameters(), lr=0.03)`

训练：

```python
num_epochs = 3
for epochin range(num_epochs):
for X, yin data_iter:
        l = loss(net(X) ,y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch{epoch + 1}, loss{l:f}')
```

训练代码中主要有几点：

1. 读取训练集和对应的标签
2. 计算损失
3. 将梯度归0
4. 进行反向传播计算梯度
5. 通过梯度优化模型中的参数
6. 在测试集（验证集）中进行验证超参数
7. 最后输出每个epoch的loss和准确率

## 3.4 Softmax回归

### 3.4.2网络架构

![image.png](%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20245f0d9df6f18075a424cb5572014c1c/image%203.png)

Softmax回归也是一个单层神经网络，计算每一个输出取决于所有输入，所以这也是一个全连接层

![image.png](%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20245f0d9df6f18075a424cb5572014c1c/image%204.png)

### 3.4.4 Softmax运算

Softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时可以让模型保持可导的性质。

           $\hat{\mathbf{y}} = \text{softmax}(\mathbf{o}) \quad \text{其中} \quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$

Softmax运算不会改变未规范化的预测之间的大小次序，只会确定分配给每个类别的概率。

### 3.4.6 损失函数

下面这个就是采用的交叉熵损失函数

$$
l(\mathbf{y}, \hat{\mathbf{y}}) = -\sum_{j=1}^{q} y_j \log \hat{y}_j.
$$

由于yi是一个长度为q的独热编码向量，故如果yi预测不正确的选项这一项将会为0，故所有的都是基于正确的选项进行计算的，并且yi是预测的概率，所以对数不会大于0.并且由于log函数的特性，这个损失函数会狠狠地**惩罚**任何概率过于低的样例，可以参考下面的图：

![image.png](%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20245f0d9df6f18075a424cb5572014c1c/image%205.png)

这个惩罚可以推动模型将正确类别的预测概率拉高

**接下来是损失函数的推导，将前面带入损失函数中，进行化简：**

$$
\begin{split}\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}\end{split}
$$

推导过程中主要有以下的几步：

1. 拆开 log：

$$
= -\sum_{j=1}^q y_j[o_j - \log\sum_{k=1}^q \exp(o_k)]
$$

1. 拆成两部分：

$$
= \log\sum_{k=1}^q\exp(o_k)-\sum_{j=1}^q y_j o_j
$$

当然了在这其中有一步就是

![image.png](%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20245f0d9df6f18075a424cb5572014c1c/image%206.png)

但是由于yi是独热向量，并且与他相关的第二项中并没有含有j相关的变量，yi的和可以为1，第二项由于是oj与j有关，故没法将其变为=1的形式

（第二项不能直接化成Oj是因为在推导时j是遍历所有类别的，只有在独热向量的数值计算时才会发生退化）

**接下来我们要解决的是对于损失函数求导的问题，下面是最终得到的结果：**

$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$

我们分别拆出两项对其进行求导：

**第一项：**

$$
\frac{\partial \log \sum_{k=1}^q e^{o_k}} {\partial o_j}
= \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} 
= \mathrm{softmax}(\mathbf{o})_j
$$

其中他的里面有关于eok对于eoj进行求导，因为是对oj求偏导，故只有在k = j时，这两项才有关，故这一项可以直接变成eoj，即上面的公式的分子

**第二项：**

$$
\frac{\partial}{\partial o_j} \left(-\sum_{i=1}^q y_i o_i\right) = -y_j
$$

关于这一项其实主要还是与上面相同，yi关于oj求偏导不变，而oj同样只有在j=k的时候生效，故求导后即为这个结果。